{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a387a4b0a766fe61",
   "metadata": {},
   "source": [
    "# Imports and Preparing the data of behavior.tsv and news.tsv #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6eba314dd9db26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:20:27.587026Z",
     "start_time": "2024-11-21T18:20:23.532950Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load the data\n",
    "base_path = Path.cwd() / 'data'\n",
    "news_path = base_path / 'news.tsv'\n",
    "behaviors_path = base_path / 'behaviors.tsv'\n",
    "\n",
    "news = pd.read_csv(news_path, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "behaviors = pd.read_csv(behaviors_path, sep='\\t', names=['impression_id', 'user_id', 'time', 'history', 'impressions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820d8667702a5dc",
   "metadata": {},
   "source": [
    "# Preprocessing with Data Cleaning Remove duplicates and Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b79cfd76900e589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:20:32.298592Z",
     "start_time": "2024-11-21T18:20:27.618186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Check if attributes such as Category, Subcategory, Title, and Abstract in news.tsv are complete.\n",
    "# Remove news items with many missing values or replace them with:\n",
    "# Category: unknown\n",
    "# Subcategory: general\n",
    "# Title and Abstract: a placeholder text like \"Missing Data.\"\n",
    "# For behaviors.tsv, remove users with missing or empty history.\n",
    "news.fillna({'category': 'unknown', 'subcategory': 'general', 'title': 'Missing Title', 'abstract': 'Missing Abstract'}, inplace=True)\n",
    "news.dropna(subset=['category', 'subcategory', 'title', 'abstract'], inplace=True)\n",
    "# behaviors.dropna(subset=['history', 'impressions'], inplace=True)\n",
    "behaviors.dropna(subset=['history'], inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "# Remove news items in news.tsv that have the same values for Title and Abstract.\n",
    "# Remove duplicate impressions (Impression ID) in behaviors.tsv\n",
    "news.drop_duplicates(subset=['title', 'abstract'], inplace=True)\n",
    "#behaviors.drop_duplicates(subset=['impression_id'], inplace=True)\n",
    "\n",
    "# Text cleaning\n",
    "# Break down Title and Abstract into tokens:\n",
    "# Remove special characters, numbers, and HTML.\n",
    "# Convert all words to lowercase.\n",
    "# Remove stopwords (e.g., using nltk or spacy).\n",
    "# Perform lemmatization to reduce words to their base form.\n",
    "def clean_text(text):\n",
    "    # Clean text by removing special characters, numbers, and converting to lowercase\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    # Remove stopwords and perform basic stemming/lemmatization\n",
    "    stopwords = set([\n",
    "        'the', 'and', 'is', 'in', 'to', 'of', 'a', 'an', 'on', 'for', 'with', 'as', 'by', 'at', 'from', 'this',\n",
    "        'that', 'it', 'or', 'but', 'not', 'be', 'are', 'was', 'were', 'can', 'will', 'would', 'should', 'has', 'have',\n",
    "        'had', 'do', 'does', 'did', 'which', 'if', 'then', 'than', 'so', 'such', 'there', 'about', 'into', 'over',\n",
    "        'after'\n",
    "    ])\n",
    "\n",
    "    def advanced_lemmatization(word):\n",
    "        # Remove common suffixes like 'ed', 'ing', 'es', 'er', 'ly'\n",
    "        suffixes = ['ed', 'ing', 'es', 'er', 'ly', 'able', 'ness']\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                return word[:-len(suffix)]\n",
    "        return word\n",
    "\n",
    "    # List of simple verbs\n",
    "    irregular_verbs = {\n",
    "        'ran': 'run',\n",
    "        'ate': 'eat',\n",
    "        'wrote': 'write',\n",
    "        'went': 'go',\n",
    "        'saw': 'see',\n",
    "        'had': 'have',\n",
    "        'was': 'be',\n",
    "        'were': 'be'\n",
    "    }\n",
    "\n",
    "    def lemmatize_irregular_verbs(word):\n",
    "        # Lemmatisiere unregelmäßige Verben\n",
    "        return irregular_verbs.get(word, word)\n",
    "\n",
    "    words = text.split()\n",
    "    lemmatized = []\n",
    "\n",
    "    for word in words:\n",
    "        # Outsort verbs\n",
    "        word = lemmatize_irregular_verbs(word)\n",
    "        # Remove suffix\n",
    "        word = advanced_lemmatization(word)\n",
    "        lemmatized.append(word)\n",
    "\n",
    "    return \" \".join(word for word in lemmatized if word not in stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Full preprocessing pipeline: clean text, remove stopwords, and lemmatize\n",
    "    cleaned_text = clean_text(text)\n",
    "    return remove_stopwords_and_lemmatize(cleaned_text)\n",
    "\n",
    "\n",
    "def preprocess_text_parallel(texts):\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "# Clean Title and Abstract\n",
    "news['clean_title'] = preprocess_text_parallel(news['title'])\n",
    "news['clean_abstract'] = preprocess_text_parallel(news['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8468aec717a4a",
   "metadata": {},
   "source": [
    "# Data Preparation with TF-IDF, One-hot and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8de5912a91b71b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:21:17.443075Z",
     "start_time": "2024-11-21T18:20:32.508414Z"
    }
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "# Convert clean_title and clean_abstract into numerical vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "tfidf_title = tfidf_vectorizer.fit_transform(news['clean_title'])\n",
    "tfidf_abstract = tfidf_vectorizer.fit_transform(news['clean_abstract'])\n",
    "\n",
    "# Combine sparse matrices\n",
    "news_features = hstack([tfidf_title, tfidf_abstract])\n",
    "\n",
    "# One-hot encode categories and subcategories\n",
    "category_encoded = pd.get_dummies(news['category'])\n",
    "subcategory_encoded = pd.get_dummies(news['subcategory'])\n",
    "\n",
    "# Combine categorical and TF-IDF features\n",
    "final_features = hstack([news_features, category_encoded.values, subcategory_encoded.values])\n",
    "\n",
    "# PCA for faster processing instead of t-SNE for better visualization\n",
    "pca = IncrementalPCA(n_components=50, batch_size=1000)\n",
    "reduced_features = pca.fit_transform(news_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79854fb457777e41",
   "metadata": {},
   "source": [
    "# KMeans Visualization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce0d2aac364305d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:54:19.960391Z",
     "start_time": "2024-11-21T18:54:16.968830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "      <td>brands queen elizabeth prince charl prince phi...</td>\n",
       "      <td>shop notebooks jackets more royals ca live wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>worst habits bel fat</td>\n",
       "      <td>these seeming harmless habits hold you back ke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "      <td>cost trump aid freeze trench ukraine war</td>\n",
       "      <td>lt ivan molchanets peek ov parapet sand bags f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "      <td>i nba wife here how affect my mental health</td>\n",
       "      <td>i felt like i fraud nba wife help fact near de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>how get rid skin tags accord dermatologist</td>\n",
       "      <td>they seem harmless very good reason you ignore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51277</th>\n",
       "      <td>N16909</td>\n",
       "      <td>weather</td>\n",
       "      <td>weathertopstories</td>\n",
       "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
       "      <td>Woolsey Fire Anniversary: A community is forev...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQJK.html</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>adapt learn soul search reflect woolsey fire</td>\n",
       "      <td>woolsey fire anniversary community forev chang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51278</th>\n",
       "      <td>N47585</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestylefamily</td>\n",
       "      <td>Family says 13-year-old Broadway star died fro...</td>\n",
       "      <td>Missing Abstract</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQYV.html</td>\n",
       "      <td>[{\"Label\": \"Broadway theatre\", \"Type\": \"F\", \"W...</td>\n",
       "      <td>[]</td>\n",
       "      <td>fami says year old broadway star di massive as...</td>\n",
       "      <td>miss abstract</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51279</th>\n",
       "      <td>N7482</td>\n",
       "      <td>sports</td>\n",
       "      <td>more_sports</td>\n",
       "      <td>St. Dominic soccer player tries to kick cancer...</td>\n",
       "      <td>Sometimes, what happens on the sidelines can b...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQnK.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>st dominic socc play tri kick canc curb</td>\n",
       "      <td>sometim what happens sidelin even more importa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51280</th>\n",
       "      <td>N34418</td>\n",
       "      <td>sports</td>\n",
       "      <td>soccer_epl</td>\n",
       "      <td>How the Sounders won MLS Cup</td>\n",
       "      <td>Mark, Jeremiah and Casey were so excited they ...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQuK.html</td>\n",
       "      <td>[{\"Label\": \"MLS Cup\", \"Type\": \"U\", \"WikidataId...</td>\n",
       "      <td>[]</td>\n",
       "      <td>how sounders won mls cup</td>\n",
       "      <td>mark jeremiah casey excit they postgame podcast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51281</th>\n",
       "      <td>N44276</td>\n",
       "      <td>autos</td>\n",
       "      <td>autossports</td>\n",
       "      <td>Best Sports Car Deals for October</td>\n",
       "      <td>Missing Abstract</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBy5rVe.html</td>\n",
       "      <td>[{\"Label\": \"Peugeot RCZ\", \"Type\": \"V\", \"Wikida...</td>\n",
       "      <td>[]</td>\n",
       "      <td>best sports car deals octob</td>\n",
       "      <td>miss abstract</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50669 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id   category        subcategory  \\\n",
       "0      N55528  lifestyle    lifestyleroyals   \n",
       "1      N19639     health         weightloss   \n",
       "2      N61837       news          newsworld   \n",
       "3      N53526     health             voices   \n",
       "4      N38324     health            medical   \n",
       "...       ...        ...                ...   \n",
       "51277  N16909    weather  weathertopstories   \n",
       "51278  N47585  lifestyle    lifestylefamily   \n",
       "51279   N7482     sports        more_sports   \n",
       "51280  N34418     sports         soccer_epl   \n",
       "51281  N44276      autos        autossports   \n",
       "\n",
       "                                                   title  \\\n",
       "0      The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                          50 Worst Habits For Belly Fat   \n",
       "2      The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3      I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4      How to Get Rid of Skin Tags, According to a De...   \n",
       "...                                                  ...   \n",
       "51277  Adapting, Learning And Soul Searching: Reflect...   \n",
       "51278  Family says 13-year-old Broadway star died fro...   \n",
       "51279  St. Dominic soccer player tries to kick cancer...   \n",
       "51280                       How the Sounders won MLS Cup   \n",
       "51281                  Best Sports Car Deals for October   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Shop the notebooks, jackets, and more that the...   \n",
       "1      These seemingly harmless habits are holding yo...   \n",
       "2      Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3      I felt like I was a fraud, and being an NBA wi...   \n",
       "4      They seem harmless, but there's a very good re...   \n",
       "...                                                  ...   \n",
       "51277  Woolsey Fire Anniversary: A community is forev...   \n",
       "51278                                   Missing Abstract   \n",
       "51279  Sometimes, what happens on the sidelines can b...   \n",
       "51280  Mark, Jeremiah and Casey were so excited they ...   \n",
       "51281                                   Missing Abstract   \n",
       "\n",
       "                                                 url  \\\n",
       "0      https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1      https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2      https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3      https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4      https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "...                                              ...   \n",
       "51277  https://assets.msn.com/labs/mind/BBWzQJK.html   \n",
       "51278  https://assets.msn.com/labs/mind/BBWzQYV.html   \n",
       "51279  https://assets.msn.com/labs/mind/BBWzQnK.html   \n",
       "51280  https://assets.msn.com/labs/mind/BBWzQuK.html   \n",
       "51281  https://assets.msn.com/labs/mind/BBy5rVe.html   \n",
       "\n",
       "                                          title_entities  \\\n",
       "0      [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1      [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                     []   \n",
       "3                                                     []   \n",
       "4      [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "...                                                  ...   \n",
       "51277  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "51278  [{\"Label\": \"Broadway theatre\", \"Type\": \"F\", \"W...   \n",
       "51279                                                 []   \n",
       "51280  [{\"Label\": \"MLS Cup\", \"Type\": \"U\", \"WikidataId...   \n",
       "51281  [{\"Label\": \"Peugeot RCZ\", \"Type\": \"V\", \"Wikida...   \n",
       "\n",
       "                                       abstract_entities  \\\n",
       "0                                                     []   \n",
       "1      [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2      [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...   \n",
       "3      [{\"Label\": \"National Basketball Association\", ...   \n",
       "4      [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "...                                                  ...   \n",
       "51277  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "51278                                                 []   \n",
       "51279                                                 []   \n",
       "51280                                                 []   \n",
       "51281                                                 []   \n",
       "\n",
       "                                             clean_title  \\\n",
       "0      brands queen elizabeth prince charl prince phi...   \n",
       "1                                   worst habits bel fat   \n",
       "2               cost trump aid freeze trench ukraine war   \n",
       "3            i nba wife here how affect my mental health   \n",
       "4             how get rid skin tags accord dermatologist   \n",
       "...                                                  ...   \n",
       "51277       adapt learn soul search reflect woolsey fire   \n",
       "51278  fami says year old broadway star di massive as...   \n",
       "51279            st dominic socc play tri kick canc curb   \n",
       "51280                           how sounders won mls cup   \n",
       "51281                        best sports car deals octob   \n",
       "\n",
       "                                          clean_abstract  cluster  \n",
       "0      shop notebooks jackets more royals ca live wit...        1  \n",
       "1      these seeming harmless habits hold you back ke...        0  \n",
       "2      lt ivan molchanets peek ov parapet sand bags f...        1  \n",
       "3      i felt like i fraud nba wife help fact near de...        1  \n",
       "4      they seem harmless very good reason you ignore...        0  \n",
       "...                                                  ...      ...  \n",
       "51277  woolsey fire anniversary community forev chang...        1  \n",
       "51278                                      miss abstract        2  \n",
       "51279  sometim what happens sidelin even more importa...        1  \n",
       "51280    mark jeremiah casey excit they postgame podcast        1  \n",
       "51281                                      miss abstract        2  \n",
       "\n",
       "[50669 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering with KMeans\n",
    "optimal_k = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "news['cluster'] = kmeans.fit_predict(reduced_features)\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e73272",
   "metadata": {},
   "source": [
    "# Start with content based recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e46272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_news = news.copy()\n",
    "# copy_news[\"cluster\"]\n",
    "\n",
    "# def find_cluster(news_df, news_id):\n",
    "#     result = news_df[news_df['news_id'] == news_id]['cluster']\n",
    "#     if not result.empty:\n",
    "#         return result.iloc[0]  # Return the cluster value\n",
    "#     else:\n",
    "#         return None  # Return None if news_id not found\n",
    "# #\n",
    "# # Example usage\n",
    "# news_id_to_find = \"some_news_id\"\n",
    "# cluster = find_cluster(copy_news, \"N41777\")\n",
    "# print(f\"The cluster for news_id {news_id_to_find} is: {cluster}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8b958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(copy_news[copy_news['news_id'] == \"N41777\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8baf4",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d83219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'history' into lists of baskets based on ids\n",
    "# Group by user and get rid of duplicates in the history\n",
    "user_histories = []\n",
    "user_data_for_df = []\n",
    "for user_id, user_data in behaviors.groupby('user_id'):\n",
    "    one_history_string = \" \".join(user_data['history'])\n",
    "    splitted_without_duplicates = set(one_history_string.split())\n",
    "    articles_list_per_user = list(splitted_without_duplicates)\n",
    "    user_histories.append(articles_list_per_user)\n",
    "    user_data_for_df.append([user_id, articles_list_per_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f590a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_dict = dict(zip(news['news_id'], news['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456201a",
   "metadata": {},
   "source": [
    "# Count article reads and sort after highest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ceeb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurence of each article\n",
    "articles_count = {}\n",
    "\n",
    "for history in user_histories:\n",
    "    for article in history:\n",
    "        if article in articles_count:\n",
    "            articles_count[article] += 1\n",
    "        else:\n",
    "            articles_count[article] = 1\n",
    "\n",
    "# Append cluster to articles count and sort clusters\n",
    "cluster_dict = defaultdict(list)\n",
    "for _, row in news.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    cluster = row['cluster']\n",
    "    \n",
    "    if news_id in articles_count:\n",
    "        cluster_dict[cluster].append((articles_count[news_id], news_id))\n",
    "\n",
    "for cluster in cluster_dict:\n",
    "    cluster_dict[cluster] = sorted(cluster_dict[cluster], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top_sorted_articles = {}\n",
    "\n",
    "for cluster, articles in cluster_dict.items():\n",
    "    top_sorted_articles[cluster] = [news_id for _, news_id in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761b4b8",
   "metadata": {},
   "source": [
    "# Create Dataframe for dealing with top cluster per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ebb10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>full_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>[N34694, N31801, N63302, N42782, N55189, N4579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U91836</td>\n",
       "      <td>[N2511, N3142, N62285, N59359, N16617, N29802,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U73700</td>\n",
       "      <td>[N25792, N24233, N47289, N26378, N21087, N1073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U34670</td>\n",
       "      <td>[N45729, N41375, N31825, N29757, N33013, N871,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U8125</td>\n",
       "      <td>[N33740, N14904, N10078, N56514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49103</th>\n",
       "      <td>U6794</td>\n",
       "      <td>[N37920, N60184, N42458, N54416, N3595, N41777...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>U23127</td>\n",
       "      <td>[N60350, N13429, N64395, N5477, N59419, N51591...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>U43157</td>\n",
       "      <td>[N30410, N12988, N62285, N14006, N24721, N1725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49106</th>\n",
       "      <td>U66493</td>\n",
       "      <td>[N56889, N50638, N34069, N26151, N22570, N5733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49107</th>\n",
       "      <td>U72015</td>\n",
       "      <td>[N53895, N48715, N5469]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                       full_history\n",
       "0      U13740  [N34694, N31801, N63302, N42782, N55189, N4579...\n",
       "1      U91836  [N2511, N3142, N62285, N59359, N16617, N29802,...\n",
       "2      U73700  [N25792, N24233, N47289, N26378, N21087, N1073...\n",
       "3      U34670  [N45729, N41375, N31825, N29757, N33013, N871,...\n",
       "4       U8125                   [N33740, N14904, N10078, N56514]\n",
       "...       ...                                                ...\n",
       "49103   U6794  [N37920, N60184, N42458, N54416, N3595, N41777...\n",
       "49104  U23127  [N60350, N13429, N64395, N5477, N59419, N51591...\n",
       "49105  U43157  [N30410, N12988, N62285, N14006, N24721, N1725...\n",
       "49106  U66493  [N56889, N50638, N34069, N26151, N22570, N5733...\n",
       "49107  U72015                            [N53895, N48715, N5469]\n",
       "\n",
       "[49108 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_histories_df = pd.DataFrame(user_data_for_df, columns=['user_id', 'full_history'])\n",
    "user_histories_df = behaviors[['user_id']].drop_duplicates().merge(user_histories_df, on='user_id')\n",
    "user_histories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d64fa5",
   "metadata": {},
   "source": [
    "# Get cluster for every article in user history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8ac7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>full_history</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>[N34694, N31801, N63302, N42782, N55189, N4579...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U91836</td>\n",
       "      <td>[N2511, N3142, N62285, N59359, N16617, N29802,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U73700</td>\n",
       "      <td>[N25792, N24233, N47289, N26378, N21087, N1073...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U34670</td>\n",
       "      <td>[N45729, N41375, N31825, N29757, N33013, N871,...</td>\n",
       "      <td>[1, 1, 0, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U8125</td>\n",
       "      <td>[N33740, N14904, N10078, N56514]</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49103</th>\n",
       "      <td>U6794</td>\n",
       "      <td>[N37920, N60184, N42458, N54416, N3595, N41777...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>U23127</td>\n",
       "      <td>[N60350, N13429, N64395, N5477, N59419, N51591...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>U43157</td>\n",
       "      <td>[N30410, N12988, N62285, N14006, N24721, N1725...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49106</th>\n",
       "      <td>U66493</td>\n",
       "      <td>[N56889, N50638, N34069, N26151, N22570, N5733...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49107</th>\n",
       "      <td>U72015</td>\n",
       "      <td>[N53895, N48715, N5469]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                       full_history  \\\n",
       "0      U13740  [N34694, N31801, N63302, N42782, N55189, N4579...   \n",
       "1      U91836  [N2511, N3142, N62285, N59359, N16617, N29802,...   \n",
       "2      U73700  [N25792, N24233, N47289, N26378, N21087, N1073...   \n",
       "3      U34670  [N45729, N41375, N31825, N29757, N33013, N871,...   \n",
       "4       U8125                   [N33740, N14904, N10078, N56514]   \n",
       "...       ...                                                ...   \n",
       "49103   U6794  [N37920, N60184, N42458, N54416, N3595, N41777...   \n",
       "49104  U23127  [N60350, N13429, N64395, N5477, N59419, N51591...   \n",
       "49105  U43157  [N30410, N12988, N62285, N14006, N24721, N1725...   \n",
       "49106  U66493  [N56889, N50638, N34069, N26151, N22570, N5733...   \n",
       "49107  U72015                            [N53895, N48715, N5469]   \n",
       "\n",
       "                                                clusters  \n",
       "0                            [1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, ...  \n",
       "2       [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]  \n",
       "3                         [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4                                           [0, 1, 1, 1]  \n",
       "...                                                  ...  \n",
       "49103                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "49104  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...  \n",
       "49105                           [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "49106               [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]  \n",
       "49107                                          [1, 1, 1]  \n",
       "\n",
       "[49108 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_histories_df[\"clusters\"] = None\n",
    "news_dict = dict(zip(news[\"news_id\"], news[\"cluster\"]))\n",
    "\n",
    "for idx, row in user_histories_df.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    full_history = row['full_history']\n",
    "    cluster_list = []\n",
    "    for article in full_history:\n",
    "        if article in news_dict:\n",
    "            cluster_list.append(news_dict[article])\n",
    "    user_histories_df.at[idx, \"clusters\"] = cluster_list\n",
    "user_histories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37fd95",
   "metadata": {},
   "source": [
    "# Important !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9357d5",
   "metadata": {},
   "source": [
    "# Some articles appear to not be in the cluster dataframe -> Look into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67d5023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id full_history clusters\n",
      "451     U9828     [N41777]       []\n",
      "9659   U59528     [N31172]       []\n",
      "12417  U73096     [N47020]       []\n",
      "12617  U71157     [N17164]       []\n",
      "16684  U86734     [N47020]       []\n",
      "17532  U24863     [N56527]       []\n",
      "23686  U14089      [N1920]       []\n",
      "27946  U60814     [N33391]       []\n",
      "28742  U45998     [N12584]       []\n",
      "34457  U30155     [N47020]       []\n",
      "41333  U12146     [N49824]       []\n",
      "48233  U29489     [N16662]       []\n"
     ]
    }
   ],
   "source": [
    "empty_list_rows = user_histories_df[user_histories_df['clusters'].apply(lambda x: isinstance(x, list) and len(x) == 0)]\n",
    "print(empty_list_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d9e79",
   "metadata": {},
   "source": [
    "# Count top cluster per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab72f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>full_history</th>\n",
       "      <th>clusters</th>\n",
       "      <th>top_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>[N34694, N31801, N63302, N42782, N55189, N4579...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U91836</td>\n",
       "      <td>[N2511, N3142, N62285, N59359, N16617, N29802,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U73700</td>\n",
       "      <td>[N25792, N24233, N47289, N26378, N21087, N1073...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U34670</td>\n",
       "      <td>[N45729, N41375, N31825, N29757, N33013, N871,...</td>\n",
       "      <td>[1, 1, 0, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U8125</td>\n",
       "      <td>[N33740, N14904, N10078, N56514]</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49103</th>\n",
       "      <td>U6794</td>\n",
       "      <td>[N37920, N60184, N42458, N54416, N3595, N41777...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>U23127</td>\n",
       "      <td>[N60350, N13429, N64395, N5477, N59419, N51591...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>U43157</td>\n",
       "      <td>[N30410, N12988, N62285, N14006, N24721, N1725...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49106</th>\n",
       "      <td>U66493</td>\n",
       "      <td>[N56889, N50638, N34069, N26151, N22570, N5733...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49107</th>\n",
       "      <td>U72015</td>\n",
       "      <td>[N53895, N48715, N5469]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49108 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                       full_history  \\\n",
       "0      U13740  [N34694, N31801, N63302, N42782, N55189, N4579...   \n",
       "1      U91836  [N2511, N3142, N62285, N59359, N16617, N29802,...   \n",
       "2      U73700  [N25792, N24233, N47289, N26378, N21087, N1073...   \n",
       "3      U34670  [N45729, N41375, N31825, N29757, N33013, N871,...   \n",
       "4       U8125                   [N33740, N14904, N10078, N56514]   \n",
       "...       ...                                                ...   \n",
       "49103   U6794  [N37920, N60184, N42458, N54416, N3595, N41777...   \n",
       "49104  U23127  [N60350, N13429, N64395, N5477, N59419, N51591...   \n",
       "49105  U43157  [N30410, N12988, N62285, N14006, N24721, N1725...   \n",
       "49106  U66493  [N56889, N50638, N34069, N26151, N22570, N5733...   \n",
       "49107  U72015                            [N53895, N48715, N5469]   \n",
       "\n",
       "                                                clusters  top_cluster  \n",
       "0                            [1, 1, 1, 1, 1, 1, 1, 1, 1]            1  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, ...            1  \n",
       "2       [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]            1  \n",
       "3                         [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]            1  \n",
       "4                                           [0, 1, 1, 1]            1  \n",
       "...                                                  ...          ...  \n",
       "49103                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]            1  \n",
       "49104  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...            1  \n",
       "49105                           [1, 1, 1, 1, 1, 1, 1, 1]            1  \n",
       "49106               [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]            1  \n",
       "49107                                          [1, 1, 1]            1  \n",
       "\n",
       "[49108 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, row in user_histories_df.iterrows():\n",
    "    cluster_counts = {}\n",
    "    clusters = row['clusters']\n",
    "    if clusters != []:\n",
    "        for cluster in clusters:\n",
    "            cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1\n",
    "        if cluster_counts:\n",
    "            top_cluster = max(cluster_counts, key=cluster_counts.get)\n",
    "    else:\n",
    "        top_cluster = None\n",
    "    user_histories_df.at[idx, \"top_cluster\"] = top_cluster\n",
    "# Get rid of this line if handling missing articles:\n",
    "user_histories_df[\"top_cluster\"] = user_histories_df[\"top_cluster\"].astype(\"Int64\")\n",
    "##\n",
    "user_histories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b743575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_cluster\n",
      "1    48158\n",
      "0      809\n",
      "2      129\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Print occurence of top clusters\n",
    "value_counts = user_histories_df[\"top_cluster\"].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a2be2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id full_history clusters  top_cluster\n",
      "451     U9828     [N41777]       []         <NA>\n",
      "9659   U59528     [N31172]       []         <NA>\n",
      "12417  U73096     [N47020]       []         <NA>\n",
      "12617  U71157     [N17164]       []         <NA>\n",
      "16684  U86734     [N47020]       []         <NA>\n",
      "17532  U24863     [N56527]       []         <NA>\n",
      "23686  U14089      [N1920]       []         <NA>\n",
      "27946  U60814     [N33391]       []         <NA>\n",
      "28742  U45998     [N12584]       []         <NA>\n",
      "34457  U30155     [N47020]       []         <NA>\n",
      "41333  U12146     [N49824]       []         <NA>\n",
      "48233  U29489     [N16662]       []         <NA>\n"
     ]
    }
   ],
   "source": [
    "rows_with_missing_values = user_histories_df[user_histories_df.isna().any(axis=1)]\n",
    "print(rows_with_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babac279",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_histories_df.dropna(subset=['top_cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed83aa",
   "metadata": {},
   "source": [
    "# Get top cluster for any user then recommend the first top 10 articles in that cluster that haven't been read by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32d6ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N26364', 'N46513', 'N2735', 'N61319', 'N22260', 'N21317', 'N21984', 'N39041', 'N40555', 'N56742']\n"
     ]
    }
   ],
   "source": [
    "# Get top cluster and recommend top 10 articles (that haven't been read)\n",
    "\"\"\"\n",
    "For 0 for example:\n",
    "For 1 for example: U23127\n",
    "For 2 for example: U83901\n",
    "\"\"\"\n",
    "user_id = \"U83901\" #\"U23127\"\n",
    "user_top_cluster = user_histories_df[user_histories_df[\"user_id\"] == user_id][\"top_cluster\"].iloc[0]\n",
    "user_history = user_histories_df[user_histories_df[\"user_id\"] == user_id][\"full_history\"].iloc[0]\n",
    "\n",
    "top_articles_from_cluster = top_sorted_articles[int(user_top_cluster)]\n",
    "\n",
    "recommend_list = []\n",
    "count = 0\n",
    "for top_article in top_articles_from_cluster:\n",
    "    if top_article not in article:\n",
    "        recommend_list.append(top_article)\n",
    "        count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "print(recommend_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b10b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What user have read so far:\n",
      "N9457\n",
      "\n",
      "What is recommended to the user:\n",
      "N26364\n",
      "N46513\n",
      "N2735\n",
      "N61319\n",
      "N22260\n",
      "N21317\n",
      "N21984\n",
      "N39041\n",
      "N40555\n",
      "N56742\n"
     ]
    }
   ],
   "source": [
    "print(\"What user have read so far:\")\n",
    "for i in user_history:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"What is recommended to the user:\")\n",
    "for i in recommend_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce640812",
   "metadata": {},
   "source": [
    "# Print titles of the recommended articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1ead2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reports: LSU LB Michael Divinity removed from team days before Alabama game\n",
      "Newly Signed Raven Makes Comeback After Losing Job, Ring\n",
      "Jimmy Garoppolo addresses Erin Andrews interview by saying he uses 'baby' 500 times a game\n",
      "After throwing a punch, Dabo Swinney made CB Andrew Booth ride the manager bus back to Clemson\n",
      "Black cat visits field during Cowboys-Giants game on 'Monday Night Football'\n",
      "New Mexico DE Nahje Flowers dies at 21\n",
      "Girl, 7, critically wounded in shooting while trick-or-treating in Little Village on Southwest Side; suspect in custody\n",
      "Six people are dead after a mass shooting in Puerto Rico\n",
      "This Is What Happens When You Take Ibuprofen Too Often, According to a Doctor\n",
      "The decisions that have backfired on the Yankees in the ALCS\n"
     ]
    }
   ],
   "source": [
    "recommend_list_text = []\n",
    "for article in recommend_list:\n",
    "    title = str(news_title_dict[article])\n",
    "    recommend_list_text.append(title)\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_CTDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
