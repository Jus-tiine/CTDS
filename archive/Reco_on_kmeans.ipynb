{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a387a4b0a766fe61",
   "metadata": {},
   "source": [
    "# Imports and Preparing the data of behavior.tsv and news.tsv #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c6eba314dd9db26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:07:14.201094Z",
     "start_time": "2024-11-24T09:07:12.756443Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load the data\n",
    "base_path = Path.cwd() / 'data'\n",
    "news_path = base_path / 'news.tsv'\n",
    "behaviors_path = base_path / 'behaviors.tsv'\n",
    "\n",
    "news = pd.read_csv(news_path, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "behaviors = pd.read_csv(behaviors_path, sep='\\t', names=['impression_id', 'user_id', 'time', 'history', 'impressions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820d8667702a5dc",
   "metadata": {},
   "source": [
    "# Preprocessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b79cfd76900e589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:22.606293Z",
     "start_time": "2024-11-24T09:07:56.650314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 613 duplicate news articles.\n",
      "Removed 0 duplicate impressions.\n",
      "Remaining duplicate news articles after cleaning: 0.\n",
      "Remaining duplicate impressions after cleaning: 0.\n",
      "TF-IDF Title Shape: (50669, 2000)\n",
      "TF-IDF Abstract Shape: (50669, 2000)\n",
      "Combined TF-IDF Features Shape: (50669, 4000)\n",
      "Category Encoding Shape: (50669, 17)\n",
      "Subcategory Encoding Shape: (50669, 263)\n",
      "Final Combined Features Shape: (50669, 4280)\n",
      "Reduced Features Shape (after PCA): (50669, 100)\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "# Check if attributes such as Category, Subcategory, Title, and Abstract in news.tsv are complete.\n",
    "# Remove news items with many missing values or replace them with:\n",
    "# Category: unknown\n",
    "# Subcategory: general\n",
    "# Title and Abstract: a placeholder text like \"Missing Data.\"\n",
    "# For behaviors.tsv, remove users with missing or empty history.\n",
    "news.fillna({'category': 'unknown', 'subcategory': 'general', 'title': 'Missing Title', 'abstract': 'Missing Abstract'}, inplace=True)\n",
    "news.dropna(subset=['category', 'subcategory', 'title', 'abstract'], inplace=True)\n",
    "behaviors.dropna(subset=['history', 'impressions'], inplace=True)\n",
    "\n",
    "# Identify duplicates in the news table based on Title and Abstract\n",
    "duplicated_news = news[news.duplicated(subset=['title', 'abstract'], keep=False)].copy()\n",
    "\n",
    "# Create a mapping of duplicate news IDs to their canonical ID\n",
    "duplicated_news['original_id'] = (\n",
    "    duplicated_news.groupby(['title', 'abstract'])['news_id']\n",
    "    .transform('first')\n",
    ")\n",
    "\n",
    "# Filter out rows where the news_id is the same as the original_id (self-matches)\n",
    "duplicate_pairs = duplicated_news[duplicated_news['news_id'] != duplicated_news['original_id']]\n",
    "\n",
    "# Create a DataFrame with two columns: 'original_id' and 'duplicate_id'\n",
    "duplicate_mapping_df = duplicate_pairs[['original_id', 'news_id']].rename(columns={'news_id': 'duplicate_id'})\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries or any desired structure if needed\n",
    "duplicate_mapping_list = duplicate_mapping_df.to_dict(orient='records')\n",
    "\n",
    "# Count duplicates before removing them\n",
    "news_duplicates_before = len(news) - len(news.drop_duplicates(subset=['title', 'abstract']))\n",
    "behaviors_duplicates_before = len(behaviors) - len(behaviors.drop_duplicates(subset=['impression_id']))\n",
    "\n",
    "# Remove duplicates\n",
    "# Remove news items in news.tsv that have the same values for Title and Abstract.\n",
    "# Remove duplicate impressions (Impression ID) in behaviors.tsv\n",
    "news.drop_duplicates(subset=['title', 'abstract'], inplace=True)\n",
    "behaviors.drop_duplicates(subset=['impression_id'], inplace=True)\n",
    "\n",
    "# Count duplicates after removing them (should be zero)\n",
    "news_duplicates_after = len(news) - len(news.drop_duplicates(subset=['title', 'abstract']))\n",
    "behaviors_duplicates_after = len(behaviors) - len(behaviors.drop_duplicates(subset=['impression_id']))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Removed {news_duplicates_before} duplicate news articles.\")\n",
    "print(f\"Removed {behaviors_duplicates_before} duplicate impressions.\")\n",
    "print(f\"Remaining duplicate news articles after cleaning: {news_duplicates_after}.\")\n",
    "print(f\"Remaining duplicate impressions after cleaning: {behaviors_duplicates_after}.\")\n",
    "\n",
    "# Erstelle einen String aus den bereinigten Titeln\n",
    "title_text = \" \".join(news['title'])\n",
    "abstract_text = \" \".join(news['abstract'])\n",
    "\n",
    "# Text cleaning\n",
    "# Break down Title and Abstract into tokens:\n",
    "# Remove special characters, numbers, and HTML.\n",
    "# Convert all words to lowercase.\n",
    "# Remove stopwords (e.g., using nltk or spacy).\n",
    "# Perform lemmatization to reduce words to their base form.\n",
    "def clean_text(text):\n",
    "    # Clean text by removing special characters, numbers, and converting to lowercase\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\d+', '<NUMBER>', text)\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    # Remove stopwords and perform basic stemming/lemmatization\n",
    "    stopwords = set([\n",
    "    'the', 'and', 'is', 'in', 'to', 'of', 'a', 'an', 'on', 'for', 'with', 'as', 'by', 'at', 'from', 'this', 'that', 'it', 'or', 'but', 'not', 'be', 'are', 'was', 'were', 'can', 'will', 'would', 'should', 'has', 'have', 'had', 'do', 'does', 'did', 'which', 'if', 'then', 'than', 'so', 'such', 'there', 'about', 'into', 'over',\n",
    "    'after', 'abc', 'accord', 'news', 'report', 'release', 'authoriti', 'people', 'act', 'aft', 'add', 'new', 'after','appear', 'he', 'she', 'they', 'we', 'you', 'i', 'his', 'her', 'their', 'its', 'ours', 'theirs', 'your', 'my', 'mine', 'ourselves', 'yourselves', 'has', 'had', 'have', 'having', 'be', 'is', 'am', 'are', 'was', 'were', 'will', 'would', 'can', 'could', 'shall', 'should', 'might', 'must', 'do', 'does', 'did', 'doing', 'done', 'doesn’t', 'didn’t', 'above', 'below', 'between', 'under', 'within', 'against', 'during', 'before', 'after', 'under', 'among', 'throughout', 'along', 'around', 'across', 'besides', 'towards', 'despite', 'except', 'as', 'like', 'by', 'nor', 'because', 'until', 'while', 'on', 'off', 'up', 'down', 'over', 'under', 'with', 'without', 'very', 'really', 'just', 'quite', 'more', 'less', 'only', 'already', 'still', 'even', 'never', 'always', 'sometimes', 'usually', 'likely', 'so', 'however', 'therefore', 'too', 'then', 'instead', 'almost', 'again', 'further', 'also', 'together', 'finally', 'actually', 'necessarily', 'let', 'let’s', 'might', 'may', 'must', 'ought', 'shall', 'should', 'could', 'would', 'can', 'can\\'t', 'didn\\'t', 'don\\'t', 'doesn\\'t', 'wasn\\'t', 'weren\\'t', 'won\\'t', 'didn\\'t', 'cnn', 'bbc', 'apple', 'microsoft', 'google', 'amazon', 'twitter', 'facebook', 'microsoft', 'tesla', 'elon', 'johnson', 'clinton', 'trump', 'obama', 'biden', 'eu', 'un', 'nato', 'fifa', 'nba', 'mlb', 'spacex', 'said', 'says', 'states', 'reported', 'reports', 'released', 'reveals', 'discusses', 'confirmed', 'announced', 'according', 'according to', 'statement', 'revealed', 'details', 'claim', 'claims', 'said', 'said to', 'called', 'describes', 'explained', 'just', 'basically', 'literally', 'actually', 'seriously', 'absolutely', 'completely', 'totally', 'clearly', 'evidently', 'definitely', 'pretty', 'exactly', 'day', 'week', 'month', 'year', 'time', 'morning', 'evening', 'night', 'today', 'tomorrow', 'yesterday', 'hour', 'minute', 'second', 'one', 'two', 'three', 'four', 'five', 'hundred', 'thousand', 'million', 'billion', 'percent', 'kg', 'm', 'cm', 'inch', 'km', 'g', 'url', 'www', 'http', 'https', 'www', 'doc', 'pdf', 'jpg', 'jpeg', 'png', 'mp3', 'zip', 'ppt', 'xls', 'fact', 'such', 'as', 'in', 'even', 'case', 'of', 'with',\n",
    "    # Add words from clusters\n",
    "    'afc', 'north', 'south', 'aaron', 'rodger', 'adam', 'gase', 'angel', 'lakers', 'alex', 'bregman', 'alexandria', 'ocasio',  'rams', 'al', 'baghdadi', 'ukraine', 'ambassador', 'action', 'afternoon','age', 'admitt', 'addition', 'agency', 'ag', 'advance', 'academy', 'additional', 'aid', 'ago',  'affect', 'afford', 'active', 'accept', 'accus', 'agre', 'advis', 'account', 'access', 'abstract', 'adults',  'adult', 'advisory', 'abandon', 'accident', 'america', 'address', 'administration', 'acr', 'ahead', 'alleged',  'airlin', 'allen', 'abuse', 'activity', 'actors', 'actress', 'agents', 'agreement', 'alarm', 'alexand', 'andy',   'alleged', 'answers', 'actor', 'allow', 'administration', 'agency', 'al', 'air', 'aim', 'alert', 'aid', 'home', 'win', 'u', 'miss', 'go', 'team', 'monday', 'tuesday', 'first', 'last', 'man', 'top', 'back', 'best', 'open', 'us', 'wednesday', 'play', 'thursday', 'sign', 'sunday', 'now', 'start', 'look', 'season', 'game', 'watch', 'want', 's', 'make', 'old', 'friday', 'saturday', 'woman'\n",
    "])\n",
    "\n",
    "    \n",
    "    def advanced_lemmatization(word):\n",
    "        # Remove common suffixes like 'ed', 'ing', 'es', 'er', 'ly'\n",
    "        suffixes = ['ed', 'ing', 'es', 'er', 'ly', 'able', 'ness']\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                return word[:-len(suffix)]\n",
    "        return word\n",
    "\n",
    "    # List of simple verbs\n",
    "    irregular_verbs = {\n",
    "        'ran': 'run',\n",
    "        'ate': 'eat',\n",
    "        'wrote': 'write',\n",
    "        'went': 'go',\n",
    "        'saw': 'see',\n",
    "        'had': 'have',\n",
    "        'was': 'be',\n",
    "        'were': 'be'\n",
    "    }\n",
    "\n",
    "    def lemmatize_irregular_verbs(word):\n",
    "        return irregular_verbs.get(word, word)\n",
    "\n",
    "    words = text.split()\n",
    "    lemmatized = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            # Outsort verbs\n",
    "            word = lemmatize_irregular_verbs(word)\n",
    "            # Remove suffix\n",
    "            word = advanced_lemmatization(word)\n",
    "            lemmatized.append(word)\n",
    "\n",
    "    return \" \".join(word for word in lemmatized if word not in stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Full preprocessing pipeline: clean text, remove stopwords, and lemmatize\n",
    "    cleaned_text = clean_text(text)\n",
    "    return remove_stopwords_and_lemmatize(cleaned_text)\n",
    "\n",
    "\n",
    "def preprocess_text_parallel(texts):\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "# Clean Title and Abstract\n",
    "news['clean_title'] = preprocess_text_parallel(news['title'])\n",
    "news['clean_abstract'] = preprocess_text_parallel(news['abstract'])\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Erstelle einen String aus den bereinigten Titeln\n",
    "title_text = \" \".join(news['clean_title'])\n",
    "abstract_text = \" \".join(news['clean_abstract'])\n",
    "\n",
    "# TF-IDF for clean_title\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=2000, stop_words='english')\n",
    "tfidf_title = tfidf_vectorizer.fit_transform(news['clean_title'])\n",
    "\n",
    "# Print dimensions after transforming titles\n",
    "print(f\"TF-IDF Title Shape: {tfidf_title.shape}\")  # Number of titles x 2000 (max_features)\n",
    "\n",
    "# TF-IDF for clean_abstract\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=2000, stop_words='english')\n",
    "tfidf_abstract = tfidf_vectorizer.fit_transform(news['clean_abstract'])\n",
    "\n",
    "# Print dimensions after transforming abstracts\n",
    "print(f\"TF-IDF Abstract Shape: {tfidf_abstract.shape}\")  # Number of abstracts x 2000 (max_features)\n",
    "\n",
    "# Combine sparse matrices (titles + abstracts)\n",
    "news_features = hstack([tfidf_title, tfidf_abstract])\n",
    "print(f\"Combined TF-IDF Features Shape: {news_features.shape}\")  # Number of items x 4000 (2000 + 2000)\n",
    "\n",
    "# One-hot encode categories and subcategories\n",
    "category_encoded = pd.get_dummies(news['category'])\n",
    "subcategory_encoded = pd.get_dummies(news['subcategory'])\n",
    "\n",
    "# Print dimensions of one-hot encoded features\n",
    "print(f\"Category Encoding Shape: {category_encoded.shape}\")  # Number of items x unique categories\n",
    "print(f\"Subcategory Encoding Shape: {subcategory_encoded.shape}\")  # Number of items x unique subcategories\n",
    "\n",
    "# Combine all features (TF-IDF + category + subcategory)\n",
    "final_features = hstack([news_features, category_encoded.values, subcategory_encoded.values])\n",
    "print(f\"Final Combined Features Shape: {final_features.shape}\")  # Number of items x (4000 + categories + subcategories)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = IncrementalPCA(n_components=100, batch_size=1000)\n",
    "reduced_features = pca.fit_transform(news_features)\n",
    "\n",
    "# Print dimensions after PCA\n",
    "print(f\"Reduced Features Shape (after PCA): {reduced_features.shape}\")  # Number of items x 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be684e5f",
   "metadata": {},
   "source": [
    "# Generating necessary dataframes and dictionaries for reccomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'history' into lists of baskets based on ids\n",
    "# Group by user and get rid of duplicates in the history\n",
    "user_histories = []\n",
    "user_data_for_df = []\n",
    "for user_id, user_data in behaviors.groupby('user_id'):\n",
    "    one_history_string = \" \".join(user_data['history'])\n",
    "    splitted_without_duplicates = set(one_history_string.split())\n",
    "    articles_list_per_user = list(splitted_without_duplicates)\n",
    "    user_histories.append(articles_list_per_user)\n",
    "    user_data_for_df.append([user_id, articles_list_per_user])\n",
    "\n",
    "# Generate dictionary for handeling the article titles\n",
    "news_title_dict = dict(zip(news['news_id'], news['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79854fb457777e41",
   "metadata": {},
   "source": [
    "# KMeans Visualization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fce0d2aac364305d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:23.061273Z",
     "start_time": "2024-11-24T09:09:22.637406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "      <td>brands queen elizabeth prince charl prince phi...</td>\n",
       "      <td>shop notebooks jackets royals ca live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>worst habits bel fat</td>\n",
       "      <td>these seeming harmless habits hold keep shedd ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "      <td>cost freeze trench war</td>\n",
       "      <td>lt ivan molchanets peek parapet sand bags fron...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "      <td>wife here how mental health</td>\n",
       "      <td>felt fraud wife help near destroy me</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>how get rid skin tags dermatologist</td>\n",
       "      <td>seem harmless good reason ignore them post how...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51277</th>\n",
       "      <td>N16909</td>\n",
       "      <td>weather</td>\n",
       "      <td>weathertopstories</td>\n",
       "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
       "      <td>Woolsey Fire Anniversary: A community is forev...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQJK.html</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>adapt learn soul search reflect woolsey fire</td>\n",
       "      <td>woolsey fire anniversary community forev chang...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51278</th>\n",
       "      <td>N47585</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestylefamily</td>\n",
       "      <td>Family says 13-year-old Broadway star died fro...</td>\n",
       "      <td>Missing Abstract</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQYV.html</td>\n",
       "      <td>[{\"Label\": \"Broadway theatre\", \"Type\": \"F\", \"W...</td>\n",
       "      <td>[]</td>\n",
       "      <td>fami broadway star di massive asthma attack</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51279</th>\n",
       "      <td>N7482</td>\n",
       "      <td>sports</td>\n",
       "      <td>more_sports</td>\n",
       "      <td>St. Dominic soccer player tries to kick cancer...</td>\n",
       "      <td>Sometimes, what happens on the sidelines can b...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQnK.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>st dominic socc tri kick canc curb</td>\n",
       "      <td>what happens sidelin important what happens fi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51280</th>\n",
       "      <td>N34418</td>\n",
       "      <td>sports</td>\n",
       "      <td>soccer_epl</td>\n",
       "      <td>How the Sounders won MLS Cup</td>\n",
       "      <td>Mark, Jeremiah and Casey were so excited they ...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQuK.html</td>\n",
       "      <td>[{\"Label\": \"MLS Cup\", \"Type\": \"U\", \"WikidataId...</td>\n",
       "      <td>[]</td>\n",
       "      <td>how sounders won mls cup</td>\n",
       "      <td>mark jeremiah casey excit postgame podcast</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51281</th>\n",
       "      <td>N44276</td>\n",
       "      <td>autos</td>\n",
       "      <td>autossports</td>\n",
       "      <td>Best Sports Car Deals for October</td>\n",
       "      <td>Missing Abstract</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBy5rVe.html</td>\n",
       "      <td>[{\"Label\": \"Peugeot RCZ\", \"Type\": \"V\", \"Wikida...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sports car deals octob</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50669 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id   category        subcategory  \\\n",
       "0      N55528  lifestyle    lifestyleroyals   \n",
       "1      N19639     health         weightloss   \n",
       "2      N61837       news          newsworld   \n",
       "3      N53526     health             voices   \n",
       "4      N38324     health            medical   \n",
       "...       ...        ...                ...   \n",
       "51277  N16909    weather  weathertopstories   \n",
       "51278  N47585  lifestyle    lifestylefamily   \n",
       "51279   N7482     sports        more_sports   \n",
       "51280  N34418     sports         soccer_epl   \n",
       "51281  N44276      autos        autossports   \n",
       "\n",
       "                                                   title  \\\n",
       "0      The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                          50 Worst Habits For Belly Fat   \n",
       "2      The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3      I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4      How to Get Rid of Skin Tags, According to a De...   \n",
       "...                                                  ...   \n",
       "51277  Adapting, Learning And Soul Searching: Reflect...   \n",
       "51278  Family says 13-year-old Broadway star died fro...   \n",
       "51279  St. Dominic soccer player tries to kick cancer...   \n",
       "51280                       How the Sounders won MLS Cup   \n",
       "51281                  Best Sports Car Deals for October   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Shop the notebooks, jackets, and more that the...   \n",
       "1      These seemingly harmless habits are holding yo...   \n",
       "2      Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3      I felt like I was a fraud, and being an NBA wi...   \n",
       "4      They seem harmless, but there's a very good re...   \n",
       "...                                                  ...   \n",
       "51277  Woolsey Fire Anniversary: A community is forev...   \n",
       "51278                                   Missing Abstract   \n",
       "51279  Sometimes, what happens on the sidelines can b...   \n",
       "51280  Mark, Jeremiah and Casey were so excited they ...   \n",
       "51281                                   Missing Abstract   \n",
       "\n",
       "                                                 url  \\\n",
       "0      https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1      https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2      https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3      https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4      https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "...                                              ...   \n",
       "51277  https://assets.msn.com/labs/mind/BBWzQJK.html   \n",
       "51278  https://assets.msn.com/labs/mind/BBWzQYV.html   \n",
       "51279  https://assets.msn.com/labs/mind/BBWzQnK.html   \n",
       "51280  https://assets.msn.com/labs/mind/BBWzQuK.html   \n",
       "51281  https://assets.msn.com/labs/mind/BBy5rVe.html   \n",
       "\n",
       "                                          title_entities  \\\n",
       "0      [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1      [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                     []   \n",
       "3                                                     []   \n",
       "4      [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "...                                                  ...   \n",
       "51277  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "51278  [{\"Label\": \"Broadway theatre\", \"Type\": \"F\", \"W...   \n",
       "51279                                                 []   \n",
       "51280  [{\"Label\": \"MLS Cup\", \"Type\": \"U\", \"WikidataId...   \n",
       "51281  [{\"Label\": \"Peugeot RCZ\", \"Type\": \"V\", \"Wikida...   \n",
       "\n",
       "                                       abstract_entities  \\\n",
       "0                                                     []   \n",
       "1      [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2      [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...   \n",
       "3      [{\"Label\": \"National Basketball Association\", ...   \n",
       "4      [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "...                                                  ...   \n",
       "51277  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "51278                                                 []   \n",
       "51279                                                 []   \n",
       "51280                                                 []   \n",
       "51281                                                 []   \n",
       "\n",
       "                                             clean_title  \\\n",
       "0      brands queen elizabeth prince charl prince phi...   \n",
       "1                                   worst habits bel fat   \n",
       "2                                 cost freeze trench war   \n",
       "3                            wife here how mental health   \n",
       "4                    how get rid skin tags dermatologist   \n",
       "...                                                  ...   \n",
       "51277       adapt learn soul search reflect woolsey fire   \n",
       "51278        fami broadway star di massive asthma attack   \n",
       "51279                 st dominic socc tri kick canc curb   \n",
       "51280                           how sounders won mls cup   \n",
       "51281                             sports car deals octob   \n",
       "\n",
       "                                          clean_abstract  cluster  \n",
       "0                  shop notebooks jackets royals ca live        3  \n",
       "1      these seeming harmless habits hold keep shedd ...        3  \n",
       "2      lt ivan molchanets peek parapet sand bags fron...        3  \n",
       "3                   felt fraud wife help near destroy me        3  \n",
       "4      seem harmless good reason ignore them post how...        3  \n",
       "...                                                  ...      ...  \n",
       "51277  woolsey fire anniversary community forev chang...        3  \n",
       "51278                                                           3  \n",
       "51279  what happens sidelin important what happens fi...        3  \n",
       "51280         mark jeremiah casey excit postgame podcast        3  \n",
       "51281                                                           3  \n",
       "\n",
       "[50669 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering with KMeans\n",
    "optimal_k = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "news['cluster'] = kmeans.fit_predict(reduced_features)\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8baf4",
   "metadata": {},
   "source": [
    "# Start Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeb81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:28.857535Z",
     "start_time": "2024-11-24T09:09:27.156833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count occurence of each article\n",
    "articles_count = {}\n",
    "\n",
    "for history in user_histories:\n",
    "    for article in history:\n",
    "        if article in articles_count:\n",
    "            articles_count[article] += 1\n",
    "        else:\n",
    "            articles_count[article] = 1\n",
    "\n",
    "# Append cluster to articles count and sort clusters from higehst to lowest\n",
    "cluster_dict = defaultdict(list)\n",
    "for _, row in news.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    cluster = row['cluster']\n",
    "    \n",
    "    if news_id in articles_count:\n",
    "        cluster_dict[cluster].append((articles_count[news_id], news_id))\n",
    "\n",
    "for cluster in cluster_dict:\n",
    "    cluster_dict[cluster] = sorted(cluster_dict[cluster], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top_sorted_articles = {}\n",
    "\n",
    "for cluster, articles in cluster_dict.items():\n",
    "    top_sorted_articles[cluster] = [news_id for _, news_id in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7ebb10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:29.023563Z",
     "start_time": "2024-11-24T09:09:28.974194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataframe for dealing with top cluster per user\n",
    "user_histories_df = pd.DataFrame(user_data_for_df, columns=['user_id', 'full_history'])\n",
    "user_histories_df = behaviors[['user_id']].drop_duplicates().merge(user_histories_df, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f8ac7fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:32.096166Z",
     "start_time": "2024-11-24T09:09:29.200895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get cluster for every article in user history\n",
    "user_histories_df[\"clusters\"] = None\n",
    "news_dict = dict(zip(news[\"news_id\"], news[\"cluster\"]))\n",
    "\n",
    "for idx, row in user_histories_df.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    full_history = row['full_history']\n",
    "    cluster_list = []\n",
    "    for article in full_history:\n",
    "        if article in news_dict:\n",
    "            cluster_list.append(news_dict[article])\n",
    "    user_histories_df.at[idx, \"clusters\"] = cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab72f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:47.996519Z",
     "start_time": "2024-11-24T09:09:44.835841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>full_history</th>\n",
       "      <th>clusters</th>\n",
       "      <th>top_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>[N31801, N45794, N18445, N34694, N10414, N5518...</td>\n",
       "      <td>[3, 5, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U91836</td>\n",
       "      <td>[N4486, N30867, N59359, N30698, N41011, N22976...</td>\n",
       "      <td>[7, 17, 5, 3, 9, 2, 3, 5, 3, 7, 1, 7, 3, 4, 15...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U73700</td>\n",
       "      <td>[N24233, N26378, N18870, N33164, N7563, N41087...</td>\n",
       "      <td>[3, 3, 3, 0, 3, 3, 3, 11, 3, 3, 3, 15, 9, 14, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U34670</td>\n",
       "      <td>[N29757, N871, N45729, N31825, N33013, N41375,...</td>\n",
       "      <td>[3, 10, 3, 3, 3, 13, 3, 6, 13, 3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U8125</td>\n",
       "      <td>[N33740, N56514, N14904, N10078]</td>\n",
       "      <td>[3, 9, 3, 3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49103</th>\n",
       "      <td>U6794</td>\n",
       "      <td>[N42458, N20059, N3595, N27448, N22058, N54416...</td>\n",
       "      <td>[0, 3, 3, 0, 3, 0, 3, 3, 13, 13, 3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>U23127</td>\n",
       "      <td>[N18073, N16874, N3127, N871, N51971, N64395, ...</td>\n",
       "      <td>[3, 4, 3, 10, 5, 3, 13, 13, 3, 11, 3, 3, 4, 7,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>U43157</td>\n",
       "      <td>[N17254, N24721, N14006, N12988, N64775, N4308...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 4, 3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49106</th>\n",
       "      <td>U66493</td>\n",
       "      <td>[N62940, N26151, N56889, N37720, N52946, N3406...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 1, 13, 3, 3, 3, 3, 18]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49107</th>\n",
       "      <td>U72015</td>\n",
       "      <td>[N48715, N5469, N53895]</td>\n",
       "      <td>[3, 3, 6]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49108 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                       full_history  \\\n",
       "0      U13740  [N31801, N45794, N18445, N34694, N10414, N5518...   \n",
       "1      U91836  [N4486, N30867, N59359, N30698, N41011, N22976...   \n",
       "2      U73700  [N24233, N26378, N18870, N33164, N7563, N41087...   \n",
       "3      U34670  [N29757, N871, N45729, N31825, N33013, N41375,...   \n",
       "4       U8125                   [N33740, N56514, N14904, N10078]   \n",
       "...       ...                                                ...   \n",
       "49103   U6794  [N42458, N20059, N3595, N27448, N22058, N54416...   \n",
       "49104  U23127  [N18073, N16874, N3127, N871, N51971, N64395, ...   \n",
       "49105  U43157  [N17254, N24721, N14006, N12988, N64775, N4308...   \n",
       "49106  U66493  [N62940, N26151, N56889, N37720, N52946, N3406...   \n",
       "49107  U72015                            [N48715, N5469, N53895]   \n",
       "\n",
       "                                                clusters  top_cluster  \n",
       "0                            [3, 5, 3, 3, 3, 3, 3, 3, 3]            3  \n",
       "1      [7, 17, 5, 3, 9, 2, 3, 5, 3, 7, 1, 7, 3, 4, 15...            3  \n",
       "2      [3, 3, 3, 0, 3, 3, 3, 11, 3, 3, 3, 15, 9, 14, ...            3  \n",
       "3                      [3, 10, 3, 3, 3, 13, 3, 6, 13, 3]            3  \n",
       "4                                           [3, 9, 3, 3]            3  \n",
       "...                                                  ...          ...  \n",
       "49103                [0, 3, 3, 0, 3, 0, 3, 3, 13, 13, 3]            3  \n",
       "49104  [3, 4, 3, 10, 5, 3, 13, 13, 3, 11, 3, 3, 4, 7,...            3  \n",
       "49105                           [3, 3, 3, 3, 3, 3, 4, 3]            3  \n",
       "49106             [3, 3, 3, 3, 3, 1, 13, 3, 3, 3, 3, 18]            3  \n",
       "49107                                          [3, 3, 6]            3  \n",
       "\n",
       "[49108 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count top cluster per user\n",
    "for idx, row in user_histories_df.iterrows():\n",
    "    cluster_counts = {}\n",
    "    clusters = row['clusters']\n",
    "    if clusters != []:\n",
    "        for cluster in clusters:\n",
    "            cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1\n",
    "        if cluster_counts:\n",
    "            top_cluster = max(cluster_counts, key=cluster_counts.get)\n",
    "    else:\n",
    "        top_cluster = None\n",
    "    user_histories_df.at[idx, \"top_cluster\"] = top_cluster\n",
    "user_histories_df[\"top_cluster\"] = user_histories_df[\"top_cluster\"].astype(\"Int64\")\n",
    "user_histories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b743575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:09:50.772764Z",
     "start_time": "2024-11-24T09:09:50.763286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_cluster\n",
      "0       316\n",
      "1       456\n",
      "2        45\n",
      "3     46844\n",
      "4       115\n",
      "5       257\n",
      "6       130\n",
      "7        96\n",
      "8        10\n",
      "9       151\n",
      "10       84\n",
      "11      108\n",
      "12       64\n",
      "13      133\n",
      "14       93\n",
      "15       66\n",
      "17       73\n",
      "18       34\n",
      "19       33\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Print occurence of top clusters\n",
    "value_counts = user_histories_df[\"top_cluster\"].value_counts().sort_index()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6ea54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T09:10:00.011377Z",
     "start_time": "2024-11-24T09:10:00.000623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What user have read so far:\n",
      "['N31801', 'N45794', 'N18445', 'N34694', 'N10414', 'N55189', 'N42782', 'N19347', 'N63302'] \n",
      "\n",
      "Recommendation for user:\n",
      "N42620: Heidi Klum's 2019 Halloween Costume Transformation Is Mind-Blowing   But, Like, What Is It?\n",
      "N31801: Joe Biden reportedly denied Communion at a South Carolina church because of his stance on abortion\n",
      "N55189: 'Wheel Of Fortune' Guest Delivers Hilarious, Off The Rails Introduction\n",
      "N43142: Former NBA first-round pick Jim Farmer arrested in sex sting operation\n",
      "N29177: Miguel Cervantes' Wife Reveals Daughter, 3, 'Died in My Arms' After Entering Hospice Care\n",
      "N16715: Mitch McConnell snubbed by Elijah Cummings' pallbearer in handshake line at U.S. Capitol ceremony\n",
      "N18870: Here Are the Biggest Deals We're Anticipating for Black Friday\n",
      "N55743: 17 photos that show the ugly truth of living in a tiny house\n",
      "N52551: Pamela Anderson gets backlash after wearing a Native American headdress for Halloween\n",
      "N61864: The News In Cartoons\n"
     ]
    }
   ],
   "source": [
    "# Get top cluster and recommend top 10 articles (that haven't been read)\n",
    "# Print recommendations\n",
    "\n",
    "user_id = \"U13740\"\n",
    "user_top_cluster = user_histories_df[user_histories_df[\"user_id\"] == user_id][\"top_cluster\"].iloc[0]\n",
    "user_history = user_histories_df[user_histories_df[\"user_id\"] == user_id][\"full_history\"].iloc[0]\n",
    "\n",
    "top_articles_from_cluster = top_sorted_articles[int(user_top_cluster)]\n",
    "\n",
    "recommend_list = []\n",
    "count = 0\n",
    "for top_article in top_articles_from_cluster:\n",
    "    if top_article not in article:\n",
    "        recommend_list.append(top_article)\n",
    "        count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "print(\"What user have read so far:\")\n",
    "print(user_history, \"\\n\")\n",
    "print(\"Recommendation for user:\")\n",
    "recommend_list_text = []\n",
    "for article in recommend_list:\n",
    "    title = str(article) + \": \" + str(news_title_dict[article])\n",
    "    recommend_list_text.append(title)\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_CTDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
