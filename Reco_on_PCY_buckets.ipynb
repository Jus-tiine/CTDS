{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Preparing the data of behavior.tsv and news.tsv #\n",
    "\n",
    "The following Notebook implements the A-priori and PCY Algorithms for the use with the MIND Dataset\n",
    "Afterwards we use the frequent item pairs and candidate pairs calculated by the PCY Algorithm to make efficient recommendations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of histories: 698365\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the data\n",
    "base_path = Path.cwd() / 'data'\n",
    "news_path = base_path / 'news.tsv'\n",
    "behaviors_path = base_path / 'behaviors.tsv'\n",
    "\n",
    "news = pd.read_csv(news_path, sep='\\t', names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "behaviors = pd.read_csv(behaviors_path, sep='\\t', names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "# Create a dictionary mapping 'news_id' to 'title'\n",
    "news_title_dict = dict(zip(news['news_id'], news['title']))\n",
    "articles_all = list(news['news_id'])\n",
    "\n",
    "# Split 'history' into lists of baskets based on ids\n",
    "# Get rid of NaNs\n",
    "behaviors.dropna(subset=['history'], inplace=True)\n",
    "# Group by user and get rid of duplicates in the history\n",
    "user_histories = []\n",
    "user_data_for_df = []\n",
    "for user_id, user_data in behaviors.groupby('user_id'):\n",
    "    one_history_string = \" \".join(user_data['history'])\n",
    "    splitted_without_duplicates = set(one_history_string.split())\n",
    "    articles_list_per_user = list(splitted_without_duplicates)\n",
    "    user_histories.append(articles_list_per_user)\n",
    "    user_data_for_df.append([user_id, articles_list_per_user])\n",
    "print(f\"Number of histories: {len(user_histories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Priori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 1\n",
    "# Store count of each individual item\n",
    "articles_count = {}\n",
    "\n",
    "for history in user_histories:\n",
    "    for article in history:\n",
    "        if article in articles_count:\n",
    "            articles_count[article] += 1\n",
    "        else:\n",
    "            articles_count[article] = 1\n",
    "\n",
    "# Find frequent items\n",
    "frequent_articles_apriori = {}\n",
    "\n",
    "threshold = 500 #50.000 histories -> 1% ist 500\n",
    "\n",
    "for article, count in articles_count.items():\n",
    "    if count >= threshold:\n",
    "        frequent_articles_apriori[article] = count\n",
    "\n",
    "# Before Pass 2\n",
    "# Create new array\n",
    "frequent_singleton_table_apriori = {}\n",
    "map_value = 1\n",
    "\n",
    "for article in articles_all:\n",
    "    if article in frequent_articles_apriori:\n",
    "        frequent_singleton_table_apriori[article] = map_value\n",
    "        map_value += 1\n",
    "    else:\n",
    "        frequent_singleton_table_apriori[article] = 0\n",
    "        \n",
    "# Pass 2\n",
    "# Store pairs of items (articles) that are both frequent (in frequent_articles)\n",
    "pairs_of_frequent_articles_apriori = {}\n",
    "\n",
    "for history in user_histories:\n",
    "    frequent_items_apriori = []\n",
    "    for article in history:\n",
    "        if article in frequent_singleton_table_apriori and frequent_singleton_table_apriori[article] > 0:\n",
    "            frequent_items_apriori.append(article)\n",
    "\n",
    "    len_freq_items = len(frequent_items_apriori)\n",
    "    for i in range(len_freq_items):\n",
    "        for j in range(i + 1, len_freq_items):\n",
    "            article1, article2 = sorted((frequent_items_apriori[i], frequent_items_apriori[j]))\n",
    "            pair = (article1, article2)\n",
    "\n",
    "            if pair in pairs_of_frequent_articles_apriori:\n",
    "                pairs_of_frequent_articles_apriori[pair] += 1\n",
    "            else:\n",
    "                pairs_of_frequent_articles_apriori[pair] = 1\n",
    "\n",
    "min_support = 500\n",
    "frequent_pairs_apriori = {}\n",
    "for pair, count in pairs_of_frequent_articles_apriori.items():\n",
    "    if count >= min_support:\n",
    "        frequent_pairs_apriori[pair] = count\n",
    "\n",
    "print(f\"Number of frequent items: {len(frequent_articles_apriori)}\")\n",
    "print(f\"Number of frequent pairs: {len(frequent_pairs_apriori)}\")         \n",
    "    \n",
    "# 3 set\n",
    "triplets_of_frequent_articles = {}\n",
    "frequent_triplets = {}\n",
    "\n",
    "for history in user_histories:\n",
    "    frequent_items_apriori = []\n",
    "    for article in history:\n",
    "        if frequent_singleton_table_apriori.get(article, 0) > 0:\n",
    "            frequent_items_apriori.append(article)\n",
    "\n",
    "    len_freq_items = len(frequent_items_apriori)\n",
    "    for i in range(len_freq_items):\n",
    "        for j in range(i + 1, len_freq_items):\n",
    "            for k in range(j + 1, len_freq_items):\n",
    "                article1, article2, article3 = sorted((frequent_items_apriori[i], frequent_items_apriori[j], frequent_items_apriori[k]))\n",
    "                triplet = (article1, article2, article3)\n",
    "\n",
    "                if article1 in frequent_singleton_table_apriori and article2 in frequent_singleton_table_apriori and article3 in frequent_singleton_table_apriori:\n",
    "                    if triplet in triplets_of_frequent_articles:\n",
    "                        triplets_of_frequent_articles[triplet] += 1\n",
    "                    else:\n",
    "                        triplets_of_frequent_articles[triplet] = 1\n",
    "\n",
    "min_support = 150\n",
    "for triplet, count in triplets_of_frequent_articles.items():\n",
    "    if count >= min_support:\n",
    "        frequent_triplets[triplet] = count\n",
    "\n",
    "print(f\"Number of frequent triplets: {len(frequent_triplets)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-Y Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent items: 282\n",
      "Number of frequent pairs: 16\n",
      "Number of candidate pairs: 29\n"
     ]
    }
   ],
   "source": [
    "# Pass 1 of the PC-Y algorithm: Count individual items and hash pairs\n",
    "# Define parameters\n",
    "min_support = 7000\n",
    "hash_table_size = 500000  # Size of the hash table\n",
    "\n",
    "# Initialize counters\n",
    "item_count = defaultdict(int)\n",
    "hash_table = [0] * hash_table_size\n",
    "\n",
    "# Count individual items and hash pairs, skipping self-pairs\n",
    "for history in user_histories:\n",
    "    for article in history:\n",
    "        item_count[article] += 1\n",
    "\n",
    "    # Generate item pairs using combinations and skip self-pairs\n",
    "    for pair in combinations(history, 2):\n",
    "        # Skip self-pairs\n",
    "        if pair[0] == pair[1]:\n",
    "            continue\n",
    "\n",
    "        hash_index = (hash(pair[0]) + hash(pair[1])) % hash_table_size\n",
    "        hash_table[hash_index] += 1\n",
    "\n",
    "# Create a bitmap based on the minimum support threshold\n",
    "bitmap = [1 if count >= min_support else 0 for count in hash_table]\n",
    "\n",
    "# Filter out infrequent items\n",
    "frequent_articles_pcy = {article for article, count in item_count.items() if count >= min_support}\n",
    "print(f\"Number of frequent items: {len(frequent_articles_pcy)}\")\n",
    "\n",
    "#Pass 2\n",
    "candidate_pairs = defaultdict(int)\n",
    "\n",
    "for history in user_histories:\n",
    "    # Filter to include only frequent items\n",
    "    filtered_basket = [article for article in history if article in frequent_articles_pcy]\n",
    "\n",
    "    for pair in combinations(filtered_basket, 2):\n",
    "        if pair[0] != pair[1] and pair[0] in frequent_articles_pcy and pair[1] in frequent_articles_pcy:\n",
    "            article1, article2 = sorted((pair[0], pair[1])) \n",
    "            hash_index = (hash(article1) + hash(article2)) % hash_table_size\n",
    "            if bitmap[hash_index]:\n",
    "                candidate_pairs[(article1, article2)] += 1\n",
    "\n",
    "#identify frequent pairs\n",
    "frequent_pairs_pcy = {pair: count for pair, count in candidate_pairs.items() if count >= min_support}\n",
    "print(f\"Number of frequent pairs: {len(frequent_pairs_pcy)}\")\n",
    "print(f\"Number of candidate pairs: {len(candidate_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System based on PCY Baskets\n",
    "\n",
    "Recommendations leveraging the capabilities of the PCY algorith.\n",
    "First we check if the input article based on which the user is supposed to get 10 recommendations, is a frequent item.\n",
    "If yes, first he gets recommended articles based on the Frequent pairs the article is part of, if it is less than 10 he gets recommeded articles based on the candidate pairs. In case that is still not enough to make 10 recommendations, the user also gets recommended the most popular articles from the same category.\n",
    "When the input article itself is not part of the frequent items, the recommendation system, recommends based on the most frequent articles from the same category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Articles:\n",
      "Article ID: N104737, Title: Kevin Spacey Won't Be Charged in Sexual Assault Case After Accuser Dies, Reason: Recommended based on frequent pair with input article.\n",
      "Article ID: N91597, Title: Heidi Klum's 2019 Halloween Costume Transformation Is Mind-Blowing   But, Like, What Is It?, Reason: Recommended based on candidate pair with input article.\n",
      "Article ID: N72571, Title: Former NFL lineman Justin Bannan arrested for attempted murder, Reason: Recommended based on candidate pair with input article.\n",
      "Article ID: N85484, Title: NFL world reacts to officials handing Packers win over Lions, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N112324, Title: Boxer Patrick Day dies after suffering traumatic brain injury in super welterweight fight, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N42718, Title: What Tom Brady, Lamar Jackson Told Each Other After Patriots-Ravens, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N66666, Title: MLB bans women who flashed their chests behind home plate during Game 5 of World Series, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N46994, Title: Frustrated Antonio Brown has active morning on Twitter, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N106403, Title: NFL Week 9 Power Rankings: More ammo for Belichick as greatest coach ever, Reason: Recommended as a frequent article in the same category.\n",
      "Article ID: N44431, Title: NFL Week 8 Power Rankings: Old-school football rules the day, Reason: Recommended as a frequent article in the same category.\n"
     ]
    }
   ],
   "source": [
    "def recommend_articles(input_article_id, num_recommendations=10):\n",
    "    recommendations = []\n",
    "    article_titles = []\n",
    "    recommendation_reasons = []  # List to store reasons for each recommendation\n",
    "    recommended_articles_set = set()  # To avoid duplicate recommendations\n",
    "\n",
    "    # Check if the input article exists\n",
    "    if input_article_id not in news['news_id'].values:\n",
    "        print(f\"Article ID {input_article_id} does not exist.\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "    # Initialize variables\n",
    "    total_recommendations = 0\n",
    "\n",
    "    # Step 1: Recommend based on frequent pairs\n",
    "    pairs_involving_input = {pair: count for pair, count in frequent_pairs_pcy.items() if input_article_id in pair}\n",
    "\n",
    "    if pairs_involving_input:\n",
    "        # Get articles paired with the input article\n",
    "        paired_articles = {}\n",
    "        for pair, count in pairs_involving_input.items():\n",
    "            other_article = pair[0] if pair[1] == input_article_id else pair[1]\n",
    "            if other_article != input_article_id:\n",
    "                paired_articles[other_article] = count\n",
    "        # Sort articles based on co-occurrence count\n",
    "        sorted_articles = sorted(paired_articles.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Add recommendations\n",
    "        for article_id, count in sorted_articles:\n",
    "            if total_recommendations < num_recommendations and article_id not in recommended_articles_set:\n",
    "                recommendations.append(article_id)\n",
    "                recommendation_reasons.append(\"Recommended based on frequent pair with input article.\")\n",
    "                recommended_articles_set.add(article_id)\n",
    "                total_recommendations += 1\n",
    "            if total_recommendations >= num_recommendations:\n",
    "                break\n",
    "\n",
    "    # Step 2: Recommend based on candidate pairs\n",
    "    if total_recommendations < num_recommendations:\n",
    "        candidate_pairs_involving_input = {pair: count for pair, count in candidate_pairs.items() if input_article_id in pair}\n",
    "        if candidate_pairs_involving_input:\n",
    "            # Get articles paired with the input article\n",
    "            candidate_articles = {}\n",
    "            for pair, count in candidate_pairs_involving_input.items():\n",
    "                other_article = pair[0] if pair[1] == input_article_id else pair[1]\n",
    "                if other_article != input_article_id and other_article not in recommended_articles_set:\n",
    "                    candidate_articles[other_article] = count\n",
    "            # Sort articles based on co-occurrence count\n",
    "            sorted_articles = sorted(candidate_articles.items(), key=lambda x: x[1], reverse=True)\n",
    "            # Add recommendations\n",
    "            for article_id, count in sorted_articles:\n",
    "                if total_recommendations < num_recommendations and article_id not in recommended_articles_set:\n",
    "                    recommendations.append(article_id)\n",
    "                    recommendation_reasons.append(\"Recommended based on candidate pair with input article.\")\n",
    "                    recommended_articles_set.add(article_id)\n",
    "                    total_recommendations += 1\n",
    "                if total_recommendations >= num_recommendations:\n",
    "                    break\n",
    "\n",
    "    # Step 3: Recommend based on article category\n",
    "    if total_recommendations < num_recommendations:\n",
    "        # Get the category of the input article\n",
    "        article_category = news.loc[news['news_id'] == input_article_id, 'category'].values[0]\n",
    "        # Filter articles in the same category\n",
    "        articles_in_category = news[news['category'] == article_category]['news_id'].tolist()\n",
    "        # Get frequency counts of articles in the same category\n",
    "        category_article_counts = {article: item_count.get(article, 0) for article in articles_in_category if article != input_article_id and article not in recommended_articles_set}\n",
    "        # Sort articles based on frequency count\n",
    "        sorted_articles = sorted(category_article_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Add recommendations\n",
    "        for article_id, count in sorted_articles:\n",
    "            if total_recommendations < num_recommendations:\n",
    "                recommendations.append(article_id)\n",
    "                recommendation_reasons.append(\"Recommended as a frequent article in the same category.\")\n",
    "                recommended_articles_set.add(article_id)\n",
    "                total_recommendations += 1\n",
    "            if total_recommendations >= num_recommendations:\n",
    "                break\n",
    "\n",
    "    # Retrieve titles for the recommended articles\n",
    "    for article_id in recommendations:\n",
    "        title = news_title_dict.get(article_id, \"Title not found\")\n",
    "        article_titles.append((article_id, title))\n",
    "\n",
    "    # Output the recommendations with reasons\n",
    "    print(\"\\nRecommended Articles:\")\n",
    "    for idx, (article_id, title) in enumerate(article_titles):\n",
    "        reason = recommendation_reasons[idx]\n",
    "        print(f\"Article ID: {article_id}, Title: {title}, Reason: {reason}\")\n",
    "\n",
    "    return article_titles\n",
    "\n",
    "input_article = 'N71977'\n",
    "recommended_articles = recommend_articles(input_article)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
